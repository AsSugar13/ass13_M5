{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "new_begining.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L6jxjK7G6eC7",
        "oxF6P69R7QF3",
        "6FErFlS67dvG",
        "Z3ilypsE67gr"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsSugar13/ass13_M5/blob/master/new_begining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXBnXqwL4YmE",
        "colab_type": "code",
        "outputId": "a15b7267-d949-44e2-b66d-4ef83f943e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jxjK7G6eC7",
        "colab_type": "text"
      },
      "source": [
        "# Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq-K1zyv3dWW",
        "colab_type": "code",
        "outputId": "b305dc7e-e83b-4a4e-c6af-1e5f006a1b33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.experimental                       import enable_iterative_imputer\n",
        "from IPython.display                            import display\n",
        "\n",
        "from sklearn.linear_model                       import LinearRegression, Ridge, Lasso, HuberRegressor, SGDRegressor\n",
        "from sklearn.svm                                import SVC, NuSVR, OneClassSVM\n",
        "from sklearn.pipeline                           import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing                      import OrdinalEncoder, FunctionTransformer, OneHotEncoder,\\\n",
        "                                                       KBinsDiscretizer, LabelEncoder, PolynomialFeatures,\\\n",
        "                                                       StandardScaler, MaxAbsScaler, maxabs_scale, OrdinalEncoder\n",
        "from imblearn.pipeline                          import Pipeline as Pipeline_IMB\n",
        "from sklearn.impute                             import SimpleImputer, IterativeImputer, KNNImputer\n",
        "from sklearn.metrics                            import make_scorer, mean_squared_error\n",
        "from scipy.stats                                import spearmanr\n",
        "from sklearn.model_selection                    import KFold, StratifiedKFold, GroupKFold, TimeSeriesSplit,\\\n",
        "                                                       RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.feature_extraction.text            import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.feature_selection                  import SelectKBest, chi2, SelectFromModel, f_regression\n",
        "from sklearn.decomposition                      import TruncatedSVD\n",
        "\n",
        "from sklearn.linear_model                       import LogisticRegression\n",
        "from sklearn.multioutput                        import MultiOutputClassifier, MultiOutputRegressor\n",
        "from sklearn.multiclass                         import OneVsRestClassifier\n",
        "from sklearn.ensemble                           import RandomForestClassifier, RandomForestRegressor,\\\n",
        "                                                       AdaBoostRegressor, AdaBoostClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model                       import ElasticNet, BayesianRidge\n",
        "from sklearn.neural_network                     import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection                    import train_test_split, RandomizedSearchCV, GridSearchCV,\\\n",
        "                                                       cross_val_score, cross_val_predict, cross_validate\n",
        "\n",
        "from imblearn                                   import FunctionSampler\n",
        "from imblearn.over_sampling                     import SMOTE, SVMSMOTE, ADASYN\n",
        "from imblearn.combine                           import SMOTETomek\n",
        "\n",
        "import gc\n",
        "import re\n",
        "import pickle\n",
        "sns.set(style=\"white\", color_codes=True)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "np.random.seed(13)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ila2hyB73dWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYp9FP163dWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('./sales_train_validation.csv', \"r\") as f:\n",
        "#     for i in range(1):\n",
        "#         print(f.readline())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ1-MbgO3dWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def macd(df, slow):\n",
        "    first, second, slow = slow*12//9, slow*26//9, slow\n",
        "    \n",
        "    first_ewm = df.ewm(span=first, min_periods=first, adjust=False, axis=1).mean().astype('float16')\n",
        "    second_ewm = df.ewm(span=second, min_periods=second, adjust=False, axis=1).mean().astype('float16')\n",
        "    \n",
        "    fast_ema = first_ewm - second_ewm\n",
        "    slow_ema = fast_ema.ewm(span=slow, min_periods=slow, adjust=False, axis=1).mean().astype('float16')\n",
        "    \n",
        "    macd = fast_ema-slow_ema\n",
        "    \n",
        "    return macd\n",
        "\n",
        "def angle(x):\n",
        "    return np.rad2deg(np.arctan(np.polyfit(range(len(x)), x, 1)[0])).astype('float16')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap5U0d2F3dW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_df(def_path='./'):\n",
        "    \n",
        "    # Load dtypes\n",
        "    with open(def_path + 'dtypes.pickle', 'rb') as f:\n",
        "        dtypes = pickle.load(f)\n",
        "        \n",
        "    df = pd.read_csv(def_path + 'sales_train_validation.csv',\n",
        "                     dtype=dtypes\n",
        "#                  , index_col='id'\n",
        "                    )\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "df = load_df(def_path='./drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw4xlwsb3dW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display(df.shape)\n",
        "# display(df.info())\n",
        "# display(df.describe())\n",
        "# display(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFu-hsbm3dXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### define dtype for the 1st time\n",
        "\n",
        "# dtypes = {}\n",
        "\n",
        "# for i, j in zip(df.dtypes.index, df.dtypes):\n",
        "#     if (j == 'int'):\n",
        "# #         df.loc[:, i] = pd.to_numeric(df.loc[:, i], downcast='integer')\n",
        "#         dtypes[i]='uint16'\n",
        "#     elif (j == 'object'):\n",
        "#         dtypes[i] = 'object'\n",
        "\n",
        "# # saving dtype for future ease importing\n",
        "# with open('dtypes.pickle', 'wb') as f:\n",
        "#     pickle.dump(dtypes, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psWblDaF3dXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nums = []\n",
        "cats = []\n",
        "\n",
        "for i, j in zip(df.dtypes.index, df.dtypes):\n",
        "    if (j == 'uint16'):\n",
        "        nums.append(i)\n",
        "    elif (j == 'object'):\n",
        "        cats.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IpZayHw3dXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### melting data\n",
        "df = df.melt(id_vars=cats, var_name='d', value_name='demand')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngUii0TM3dXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adding_cols(df, lags=[0, 90, 180], macd_angle=False, target_col='demand'):\n",
        "    \n",
        "#     df = df.copy()\n",
        "    \n",
        "    params = []\n",
        "\n",
        "    for lag in lags:\n",
        "        print(lag)\n",
        "    \n",
        "        #rolling mean\n",
        "        for window in [7, 14]:\n",
        "\n",
        "            add_param = df.groupby(by='item_id')[target_col]\\\n",
        "            .transform(lambda x: x.shift(lag))\\\n",
        "            .rolling(window).mean().astype('float16')\n",
        "            \n",
        "            sub_column_name = f'rol_mean_t{window}_l{lag}'\n",
        "            params.append(sub_column_name)\n",
        "            df.loc[:, sub_column_name] = add_param\n",
        "            gc.collect()\n",
        "\n",
        "        #rolling std\n",
        "        for window in [7, 14]:\n",
        "            \n",
        "            add_param = df.groupby(by='item_id')[target_col]\\\n",
        "            .transform(lambda x: x.shift(lag))\\\n",
        "            .rolling(window).std().astype('float16')\n",
        "            \n",
        "            sub_column_name = f'rol_std_t{window}_l{lag}'\n",
        "            params.append(sub_column_name)\n",
        "            df.loc[:, sub_column_name] = add_param\n",
        "            gc.collect()\n",
        "            \n",
        "    #     #rolling max\n",
        "    #     for window in [7, 14, 30]:\n",
        "            \n",
        "    #         add_param = df.groupby(by='item_id')[target_col]\\\n",
        "    #         .transform(lambda x: x.shift(lag))\\\n",
        "    #         .rolling(window).max().astype('float16')\n",
        "            \n",
        "    #         sub_column_name = f'rol_max_t{window}_l{lag}'\n",
        "    #         params.append(sub_column_name)\n",
        "    #         df.loc[:, sub_column_name] = add_param\n",
        "    #         gc.collect()\n",
        "            \n",
        "    #     #rolling min\n",
        "    #     for window in [7, 14, 30]:\n",
        "            \n",
        "    #         add_param = df.groupby(by='item_id')[target_col]\\\n",
        "    #         .transform(lambda x: x.shift(lag))\\\n",
        "    #         .rolling(window).min().astype('float16')\n",
        "            \n",
        "    #         sub_column_name = f'rol_min_t{window}_l{lag}'\n",
        "    #         params.append(sub_column_name)\n",
        "    #         df.loc[:, sub_column_name] = add_param\n",
        "    #         gc.collect()\n",
        "            \n",
        "    #     #rolling skew\n",
        "    #     for window in [7, 14, 30]:\n",
        "            \n",
        "    #         add_param = df.groupby(by='item_id')[target_col]\\\n",
        "    #         .transform(lambda x: x.shift(lag))\\\n",
        "    #         .rolling(window).skew().astype('float16')\n",
        "            \n",
        "    #         sub_column_name = f'rol_skew_t{window}_l{lag}'\n",
        "    #         params.append(sub_column_name)\n",
        "    #         df.loc[:, sub_column_name] = add_param\n",
        "    #         gc.collect()\n",
        "            \n",
        "    #     #rolling kurt\n",
        "    #     for window in [7, 14, 30]:\n",
        "            \n",
        "    #         add_param = df.groupby(by='item_id')[target_col]\\\n",
        "    #         .transform(lambda x: x.shift(lag))\\\n",
        "    #         .rolling(window).kurt().astype('float16')\n",
        "            \n",
        "    #         sub_column_name = f'rol_kurt_t{window}_l{lag}'\n",
        "    #         params.append(sub_column_name)\n",
        "    #         df.loc[:, sub_column_name] = add_param\n",
        "    #         gc.collect()\n",
        "            \n",
        "        #ewm mean    \n",
        "        for window in [7, 14]:\n",
        "            \n",
        "            add_param = df.groupby(by='item_id')[target_col]\\\n",
        "            .transform(lambda x: x.shift(lag))\\\n",
        "            .ewm(window).mean().astype('float16')\n",
        "            \n",
        "            sub_column_name = f'ewm_mean_t{window}_l{lag}'\n",
        "            params.append(sub_column_name)\n",
        "            df.loc[:, sub_column_name] = add_param\n",
        "            gc.collect()\n",
        "            \n",
        "        #ewm std    \n",
        "        for window in [7, 14]:\n",
        "            \n",
        "            add_param = df.groupby(by='item_id')[target_col]\\\n",
        "            .transform(lambda x: x.shift(lag))\\\n",
        "            .ewm(window).std().astype('float16')\n",
        "            \n",
        "            sub_column_name = f'ewm_std_t{window}_l{lag}'\n",
        "            params.append(sub_column_name)\n",
        "            df.loc[:, sub_column_name] = add_param\n",
        "            gc.collect()\n",
        "\n",
        "    #     #macd\n",
        "    #     for short in [4, 7, 9]:\n",
        "    #         add_param = macd(df[nums], short)\n",
        "    #         sub_column_name = f'macd_'+str(short)\n",
        "    #         params.append(sub_column_name)\n",
        "    #         add_param.columns = pd.MultiIndex.from_tuples([(col[0], sub_column_name) for col in add_param.columns])\n",
        "    #         df_out = df_out.join(add_param).sort_index(axis=1)\n",
        "    #         gc.collect()\n",
        "\n",
        "    #         if macd_angle==True:\n",
        "    #             for window in [2, 3, 5]:\n",
        "    #                 add_param = add_param.rolling(window, axis=1).apply(angle, raw=True)\n",
        "    #                 sub_column_name = f'macd_'+str(short)+'_angle_'+str(window)\n",
        "    #                 params.append(sub_column_name)\n",
        "    #                 add_param.columns = pd.MultiIndex.from_tuples([(col[0], sub_column_name) for col in add_param.columns])\n",
        "    #                 df_out = df_out.join(add_param).sort_index(axis=1)\n",
        "    #                 gc.collect()\n",
        "\n",
        "    # gc.collect()\n",
        "    return df, params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uet_-b8n3dXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_calendar(def_path='./'):  \n",
        "    calendar = pd.read_csv(def_path + 'calendar.csv',\n",
        "                           \n",
        "                           usecols=['wday', 'month', 'event_name_1', 'event_type_1',\n",
        "                                    'event_name_2', 'event_type_2',\n",
        "                                    'snap_CA', 'snap_TX', 'snap_WI', 'd'],\n",
        "\n",
        "                           dtype={'wday': 'uint8',\n",
        "                                  'month': 'uint8',\n",
        "                                  'snap_CA': 'uint8',\n",
        "                                  'snap_TX': 'uint8',\n",
        "                                  'snap_WI': 'uint8',                            \n",
        "                                 }\n",
        "                          )\n",
        "    \n",
        "    calendar.fillna('Nothing', inplace=True)\n",
        "    # calendar.drop(['date', 'wm_yr_wk', 'weekday','year'])\n",
        "    \n",
        "    cats_to_encode = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "    \n",
        "#####################################################################################################    \n",
        "    event_name_1_in_1d =(calendar['event_name_1'].shift(-1, axis=0).fillna('Nothing')!='Nothing')\n",
        "    event_name_1_in_1d.name = 'event_name_1_in_1d'\n",
        "    cats_to_encode.append('event_name_1_in_1d')\n",
        "    \n",
        "    event_name_1_in_3d =(calendar['event_name_1'].shift(-1, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-2, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-3, axis=0).fillna('Nothing')!='Nothing')\n",
        "    event_name_1_in_3d.name = 'event_name_1_in_3d'\n",
        "    cats_to_encode.append('event_name_1_in_3d')\n",
        "\n",
        "    event_name_1_in_7d =(calendar['event_name_1'].shift(-1, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-2, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-3, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-4, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-5, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-6, axis=0).fillna('Nothing')!='Nothing')|\\\n",
        "                        (calendar['event_name_1'].shift(-7, axis=0).fillna('Nothing')!='Nothing')\n",
        "    event_name_1_in_7d.name = 'event_name_1_in_7d'\n",
        "    cats_to_encode.append('event_name_1_in_7d')\n",
        "    \n",
        "    calendar = pd.concat([calendar, event_name_1_in_1d, event_name_1_in_3d, event_name_1_in_7d], axis=1)\n",
        "#####################################################################################################\n",
        "\n",
        "#params\n",
        "    params = calendar.columns.tolist()\n",
        "\n",
        "#encoding  cats\n",
        "    encoder = OrdinalEncoder(dtype='uint8')    \n",
        "    calendar.loc[:, cats_to_encode] = encoder.fit_transform(calendar[cats_to_encode])\n",
        "    for i in cats_to_encode:\n",
        "        calendar.loc[:, i] = calendar.loc[:, i].astype('uint8')\n",
        "    \n",
        "    return calendar, params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAQ0fcrtGwJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# df.loc[:, 'demand_log'] = df['demand'].apply(lambda x: np.log1p(x)).astype('float16')\n",
        "# df, params_df = adding_cols(df, lags=[0, 30, 180], target_col='demand_log')\n",
        "# calendar, params_calendar = load_calendar(def_path='./drive/My Drive/Colab Notebooks/')\n",
        "\n",
        "# df = df.merge(calendar, how='left', on='d')\n",
        "# del calendar\n",
        "\n",
        "# df.dropna(axis=0, inplace=True)\n",
        "# print('Completed')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7V0yvjK3dXf",
        "colab_type": "code",
        "outputId": "3458ea29-c15a-481a-c3e7-d37384fd5bdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "df, params_df = adding_cols(df, lags=[0, 30, 180])\n",
        "calendar, params_calendar = load_calendar(def_path='./drive/My Drive/Colab Notebooks/')\n",
        "\n",
        "df = df.merge(calendar, how='left', on='d')\n",
        "del calendar\n",
        "\n",
        "df.dropna(axis=0, inplace=True)\n",
        "print('Completed')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "30\n",
            "180\n",
            "Completed\n",
            "CPU times: user 10min 30s, sys: 6.13 s, total: 10min 36s\n",
            "Wall time: 10min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxF6P69R7QF3",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUleAoKYDjpq",
        "colab_type": "code",
        "outputId": "b35ca6bf-d8e0-44d9-df1a-5c16ba72f634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "print(gpu_info)\n",
        "\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# print(!lscpu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 23 19:05:47 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XJgNSnJnTjL",
        "colab_type": "code",
        "outputId": "c37de117-1f0e-4346-b3a0-138fd2785528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "display(df.head())\n",
        "\n",
        "display(df.id.value_counts().shape)\n",
        "display(df.item_id.value_counts().shape)\n",
        "display(df.dept_id.value_counts().shape)\n",
        "display(df.cat_id.value_counts().shape)\n",
        "display(df.store_id.value_counts().shape)\n",
        "display(df.state_id.value_counts().shape)\n",
        "display(df.d.value_counts().shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>demand</th>\n",
              "      <th>rol_mean_t7_l0</th>\n",
              "      <th>rol_mean_t14_l0</th>\n",
              "      <th>rol_std_t7_l0</th>\n",
              "      <th>rol_std_t14_l0</th>\n",
              "      <th>ewm_mean_t7_l0</th>\n",
              "      <th>ewm_mean_t14_l0</th>\n",
              "      <th>ewm_std_t7_l0</th>\n",
              "      <th>ewm_std_t14_l0</th>\n",
              "      <th>rol_mean_t7_l30</th>\n",
              "      <th>rol_mean_t14_l30</th>\n",
              "      <th>rol_std_t7_l30</th>\n",
              "      <th>rol_std_t14_l30</th>\n",
              "      <th>ewm_mean_t7_l30</th>\n",
              "      <th>ewm_mean_t14_l30</th>\n",
              "      <th>ewm_std_t7_l30</th>\n",
              "      <th>ewm_std_t14_l30</th>\n",
              "      <th>rol_mean_t7_l180</th>\n",
              "      <th>rol_mean_t14_l180</th>\n",
              "      <th>rol_std_t7_l180</th>\n",
              "      <th>rol_std_t14_l180</th>\n",
              "      <th>ewm_mean_t7_l180</th>\n",
              "      <th>ewm_mean_t14_l180</th>\n",
              "      <th>ewm_std_t7_l180</th>\n",
              "      <th>ewm_std_t14_l180</th>\n",
              "      <th>wday</th>\n",
              "      <th>month</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>snap_CA</th>\n",
              "      <th>snap_TX</th>\n",
              "      <th>snap_WI</th>\n",
              "      <th>event_name_1_in_1d</th>\n",
              "      <th>event_name_1_in_3d</th>\n",
              "      <th>event_name_1_in_7d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>548833</th>\n",
              "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_014</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.285645</td>\n",
              "      <td>0.142822</td>\n",
              "      <td>0.755859</td>\n",
              "      <td>0.534668</td>\n",
              "      <td>0.389893</td>\n",
              "      <td>0.977051</td>\n",
              "      <td>1.977539</td>\n",
              "      <td>4.632812</td>\n",
              "      <td>1.286133</td>\n",
              "      <td>0.643066</td>\n",
              "      <td>1.704102</td>\n",
              "      <td>1.335938</td>\n",
              "      <td>1.311523</td>\n",
              "      <td>2.021484</td>\n",
              "      <td>3.560547</td>\n",
              "      <td>5.007812</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.472656</td>\n",
              "      <td>3.210938</td>\n",
              "      <td>0.947754</td>\n",
              "      <td>1.005859</td>\n",
              "      <td>3.138672</td>\n",
              "      <td>3.214844</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548834</th>\n",
              "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_015</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.285645</td>\n",
              "      <td>0.142822</td>\n",
              "      <td>0.755859</td>\n",
              "      <td>0.534668</td>\n",
              "      <td>0.341064</td>\n",
              "      <td>0.912109</td>\n",
              "      <td>1.854492</td>\n",
              "      <td>4.484375</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.714355</td>\n",
              "      <td>1.527344</td>\n",
              "      <td>1.326172</td>\n",
              "      <td>1.273438</td>\n",
              "      <td>1.953125</td>\n",
              "      <td>3.332031</td>\n",
              "      <td>4.843750</td>\n",
              "      <td>0.856934</td>\n",
              "      <td>1.286133</td>\n",
              "      <td>1.573242</td>\n",
              "      <td>3.291016</td>\n",
              "      <td>1.388672</td>\n",
              "      <td>1.315430</td>\n",
              "      <td>3.107422</td>\n",
              "      <td>3.179688</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548835</th>\n",
              "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_016</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.285645</td>\n",
              "      <td>0.142822</td>\n",
              "      <td>0.755859</td>\n",
              "      <td>0.534668</td>\n",
              "      <td>0.298340</td>\n",
              "      <td>0.851074</td>\n",
              "      <td>1.738281</td>\n",
              "      <td>4.339844</td>\n",
              "      <td>1.713867</td>\n",
              "      <td>1.357422</td>\n",
              "      <td>3.302734</td>\n",
              "      <td>2.560547</td>\n",
              "      <td>2.238281</td>\n",
              "      <td>2.421875</td>\n",
              "      <td>4.085938</td>\n",
              "      <td>5.011719</td>\n",
              "      <td>1.286133</td>\n",
              "      <td>1.642578</td>\n",
              "      <td>2.214844</td>\n",
              "      <td>3.410156</td>\n",
              "      <td>1.900391</td>\n",
              "      <td>1.683594</td>\n",
              "      <td>3.162109</td>\n",
              "      <td>3.220703</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548836</th>\n",
              "      <td>HOBBIES_1_017_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_017</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142822</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.534668</td>\n",
              "      <td>0.261230</td>\n",
              "      <td>0.794434</td>\n",
              "      <td>1.629883</td>\n",
              "      <td>4.195312</td>\n",
              "      <td>1.713867</td>\n",
              "      <td>1.357422</td>\n",
              "      <td>3.302734</td>\n",
              "      <td>2.560547</td>\n",
              "      <td>1.958984</td>\n",
              "      <td>2.261719</td>\n",
              "      <td>3.900391</td>\n",
              "      <td>4.878906</td>\n",
              "      <td>1.286133</td>\n",
              "      <td>1.642578</td>\n",
              "      <td>2.214844</td>\n",
              "      <td>3.410156</td>\n",
              "      <td>1.635742</td>\n",
              "      <td>1.520508</td>\n",
              "      <td>3.007812</td>\n",
              "      <td>3.099609</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548837</th>\n",
              "      <td>HOBBIES_1_018_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_018</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142822</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.534668</td>\n",
              "      <td>0.228516</td>\n",
              "      <td>0.741699</td>\n",
              "      <td>1.527344</td>\n",
              "      <td>4.058594</td>\n",
              "      <td>1.713867</td>\n",
              "      <td>1.357422</td>\n",
              "      <td>3.302734</td>\n",
              "      <td>2.560547</td>\n",
              "      <td>1.713867</td>\n",
              "      <td>2.111328</td>\n",
              "      <td>3.708984</td>\n",
              "      <td>4.750000</td>\n",
              "      <td>1.286133</td>\n",
              "      <td>1.642578</td>\n",
              "      <td>2.214844</td>\n",
              "      <td>3.410156</td>\n",
              "      <td>1.411133</td>\n",
              "      <td>1.377930</td>\n",
              "      <td>2.851562</td>\n",
              "      <td>2.982422</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   id        item_id    dept_id   cat_id store_id state_id     d  demand  rol_mean_t7_l0  rol_mean_t14_l0  rol_std_t7_l0  rol_std_t14_l0  ewm_mean_t7_l0  ewm_mean_t14_l0  ewm_std_t7_l0  ewm_std_t14_l0  rol_mean_t7_l30  rol_mean_t14_l30  rol_std_t7_l30  rol_std_t14_l30  ewm_mean_t7_l30  ewm_mean_t14_l30  ewm_std_t7_l30  ewm_std_t14_l30  rol_mean_t7_l180  rol_mean_t14_l180  rol_std_t7_l180  rol_std_t14_l180  ewm_mean_t7_l180  ewm_mean_t14_l180  ewm_std_t7_l180  ewm_std_t14_l180  wday  month  event_name_1  event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  event_name_1_in_1d  event_name_1_in_3d  event_name_1_in_7d\n",
              "548833  HOBBIES_1_014_CA_1_validation  HOBBIES_1_014  HOBBIES_1  HOBBIES     CA_1       CA  d_19       0        0.285645         0.142822       0.755859        0.534668        0.389893         0.977051       1.977539        4.632812         1.286133          0.643066        1.704102         1.335938         1.311523          2.021484        3.560547         5.007812          2.000000           1.000000         4.472656          3.210938          0.947754           1.005859         3.138672          3.214844     5      2            19             2             3             1        0        0        0                   0                   0                   1\n",
              "548834  HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES     CA_1       CA  d_19       0        0.285645         0.142822       0.755859        0.534668        0.341064         0.912109       1.854492        4.484375         1.000000          0.714355        1.527344         1.326172         1.273438          1.953125        3.332031         4.843750          0.856934           1.286133         1.573242          3.291016          1.388672           1.315430         3.107422          3.179688     5      2            19             2             3             1        0        0        0                   0                   0                   1\n",
              "548835  HOBBIES_1_016_CA_1_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     CA_1       CA  d_19       0        0.285645         0.142822       0.755859        0.534668        0.298340         0.851074       1.738281        4.339844         1.713867          1.357422        3.302734         2.560547         2.238281          2.421875        4.085938         5.011719          1.286133           1.642578         2.214844          3.410156          1.900391           1.683594         3.162109          3.220703     5      2            19             2             3             1        0        0        0                   0                   0                   1\n",
              "548836  HOBBIES_1_017_CA_1_validation  HOBBIES_1_017  HOBBIES_1  HOBBIES     CA_1       CA  d_19       0        0.000000         0.142822       0.000000        0.534668        0.261230         0.794434       1.629883        4.195312         1.713867          1.357422        3.302734         2.560547         1.958984          2.261719        3.900391         4.878906          1.286133           1.642578         2.214844          3.410156          1.635742           1.520508         3.007812          3.099609     5      2            19             2             3             1        0        0        0                   0                   0                   1\n",
              "548837  HOBBIES_1_018_CA_1_validation  HOBBIES_1_018  HOBBIES_1  HOBBIES     CA_1       CA  d_19       0        0.000000         0.142822       0.000000        0.534668        0.228516         0.741699       1.527344        4.058594         1.713867          1.357422        3.302734         2.560547         1.713867          2.111328        3.708984         4.750000          1.286133           1.642578         2.214844          3.410156          1.411133           1.377930         2.851562          2.982422     5      2            19             2             3             1        0        0        0                   0                   0                   1"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(30490,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3049,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(7,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1895,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhckDPOgPm7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcfFtz8NnRrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emb_init(x):\n",
        "    x = x.weight.data\n",
        "    std = 2/(x.size(1)+1)\n",
        "    torch.nn.init.normal_(x, mean=0, std=std)\n",
        "    # sc = 2/(x.size(1)+1)\n",
        "    # x.uniform_(-sc,sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ52S7zqe0pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ninp, batch_size, nlayers=8, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn1 = nn.LSTM(ninp, 32, nlayers, dropout=dropout)\n",
        "        self.fc1 = nn.Linear(32, 16)\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "        # self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.ninp = ninp\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(len(x), 1, -1)\n",
        "        x, hid = self.rnn1(x)\n",
        "        x = self.dropout(self.fc1(x).sum(1))\n",
        "        # x = self.dropout(self.fc1(x))\n",
        "        x = self.dropout(self.fc2(x))\n",
        "        # x = self.dropout(self.fc3(x))\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKIjMGmhmmgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_szs, batch_size=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(r, c) for r, c in emb_szs])\n",
        "        for i in self.embeddings: emb_init(i)\n",
        "\n",
        "        emb_len = sum(i.embedding_dim for i in self.embeddings)\n",
        "\n",
        "        self.fc1 = nn.Linear(emb_len, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [emb(x[:, i].long()) for i, emb in enumerate(self.embeddings)]\n",
        "        x = torch.cat(x, dim=1)\n",
        "        # x = self.drop(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        # x = self.dropout(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # x = self.dropout(self.fc3(x))\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz6Pbn2ib_QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_Emb_Model(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, emb_szs, cont_len, nlayers=8, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bn = nn.BatchNorm1d(cont_len)\n",
        "\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(r, c) for r, c in emb_szs])\n",
        "        for i in self.embeddings: emb_init(i)\n",
        "        emb_len = sum(i.embedding_dim for i in self.embeddings)\n",
        "        ninp = emb_len + cont_len\n",
        "\n",
        "        self.rnn1 = nn.LSTM(ninp, 8, nlayers, dropout=dropout)\n",
        "        # self.fc0 = nn.Linear(ninp, 64)\n",
        "\n",
        "        self.fc1 = nn.Linear(8, 1)\n",
        "        # self.fc2 = nn.Linear(16, 1)\n",
        "        # self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x_cont, x_cat):\n",
        "\n",
        "        x_cat = [emb(x_cat[:, i].long()) for i, emb in enumerate(self.embeddings)]\n",
        "        x_cat = torch.cat(x_cat, dim=1)\n",
        "        x_cont = self.bn(x_cont)\n",
        "        x = self.dropout(torch.cat([x_cont, x_cat], dim=1))\n",
        "        x = x.view(len(x), 1, -1)\n",
        "\n",
        "        x, hid = self.rnn1(x)\n",
        "        x = self.dropout(self.fc1(x).sum(1))\n",
        "\n",
        "        # x = self.dropout(self.fc0(x))\n",
        "        # x = self.dropout(self.fc1(x))\n",
        "        # x = self.dropout(self.fc2(x))\n",
        "\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tj5BIQDD_I9M",
        "colab": {}
      },
      "source": [
        "class LSTM_Emb_Model_2(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, emb_szs, cont_len, nlayers=7, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bn = nn.BatchNorm1d(cont_len)\n",
        "\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(r, c) for r, c in emb_szs])\n",
        "        for i in self.embeddings: emb_init(i)\n",
        "        emb_len = sum(i.embedding_dim for i in self.embeddings)\n",
        "        ninp = emb_len + cont_len\n",
        "\n",
        "        self.rnn1 = nn.LSTM(cont_len, 14, nlayers, dropout=dropout, bidirectional=True)\n",
        "        # self.fc_cats = nn.Linear(emb_len, 1)\n",
        "        self.fc1 = nn.Linear(emb_len+28, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x_cont, x_cat):\n",
        "\n",
        "        x_cat = [emb(x_cat[:, i].long()) for i, emb in enumerate(self.embeddings)]\n",
        "        x_cat = torch.cat(x_cat, dim=1)\n",
        "        # x_cat = self.dropout(self.fc_cats(x_cat))\n",
        "\n",
        "        x_cont = self.bn(x_cont)\n",
        "        x_cont = x_cont.view(len(x_cont), 1, -1)\n",
        "        x_cont, hid = self.rnn1(x_cont)\n",
        "        x_cont = x_cont.sum(1)\n",
        "\n",
        "        x = self.dropout(torch.cat([x_cont, x_cat], dim=1))\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jMOJ5EhNw4Na",
        "colab": {}
      },
      "source": [
        "class RNN_Emb_Model(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, emb_szs, cont_len, nlayers=7, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bn = nn.BatchNorm1d(cont_len)\n",
        "\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(r, c) for r, c in emb_szs])\n",
        "        for i in self.embeddings: emb_init(i)\n",
        "        emb_len = sum(i.embedding_dim for i in self.embeddings)\n",
        "        ninp = emb_len + cont_len\n",
        "\n",
        "        self.rnn1 = nn.RNN(cont_len, 90, nlayers, dropout=dropout, bidirectional=True)\n",
        "        # self.fc_cats = nn.Linear(emb_len, 1)\n",
        "        self.fc1 = nn.Linear(emb_len+90*2, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x_cont, x_cat):\n",
        "\n",
        "        x_cat = [emb(x_cat[:, i].long()) for i, emb in enumerate(self.embeddings)]\n",
        "        x_cat = torch.cat(x_cat, dim=1)\n",
        "        # x_cat = self.dropout(self.fc_cats(x_cat))\n",
        "\n",
        "        x_cont = self.bn(x_cont)\n",
        "        x_cont = x_cont.view(len(x_cont), 1, -1)\n",
        "        x_cont, hid = self.rnn1(x_cont)\n",
        "        x_cont = x_cont.sum(1)\n",
        "\n",
        "        x = self.dropout(torch.cat([x_cont, x_cat], dim=1))\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gzu6xjlkY2V0",
        "colab": {}
      },
      "source": [
        "class RNN_Emb_Exp_Model(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, emb_szs, cont_len, nlayers=7, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bn = nn.BatchNorm1d(cont_len)\n",
        "\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(r, c) for r, c in emb_szs])\n",
        "        for i in self.embeddings: emb_init(i)\n",
        "        emb_len = sum(i.embedding_dim for i in self.embeddings)\n",
        "        ninp = emb_len + cont_len\n",
        "\n",
        "        self.rnn1 = nn.RNN(cont_len, 30, nlayers, dropout=dropout, bidirectional=True)\n",
        "        # self.fc_cats = nn.Linear(emb_len, 1)\n",
        "        self.fc1 = nn.Linear(emb_len+30*2, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x_cont, x_cat):\n",
        "\n",
        "        x_cat = [emb(x_cat[:, i].long()) for i, emb in enumerate(self.embeddings)]\n",
        "        x_cat = torch.cat(x_cat, dim=1)\n",
        "        # x_cat = self.dropout(self.fc_cats(x_cat))\n",
        "\n",
        "        # x_cont = torch.log1p(x_cont)\n",
        "        x_cont = self.bn(x_cont)\n",
        "        x_cont = x_cont.view(len(x_cont), 1, -1)\n",
        "        x_cont, hid = self.rnn1(x_cont)\n",
        "        x_cont = x_cont.sum(1)\n",
        "        # x_cont = torch.expm1(x_cont)\n",
        "\n",
        "        x = self.dropout(torch.cat([x_cont, x_cat], dim=1))\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtAKaZMDQt8V",
        "colab_type": "text"
      },
      "source": [
        "#Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FErFlS67dvG",
        "colab_type": "text"
      },
      "source": [
        "#Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApBXRDGKlneA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nums = params_df\n",
        "cats = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'wday', 'month', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "bools = ['snap_CA', 'snap_TX', 'snap_WI', 'event_name_1_in_1d', 'event_name_1_in_3d', 'event_name_1_in_7d']\n",
        "\n",
        "get_cat = FunctionTransformer(lambda df: df.loc[:, cats], validate=False)\n",
        "get_num = FunctionTransformer(lambda df: df.loc[:, nums], validate=False)\n",
        "get_bool = FunctionTransformer(lambda df: df.loc[:, bools], validate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv-0OMTeUEfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#masks for nn eval\n",
        "test_edge_start = 1886 #1913-28+1\n",
        "test_edge_end = 1913 #1913\n",
        "\n",
        "test_mask_start_ind = df[df.d == 'd_'+str(test_edge_start)].index[0]\n",
        "test_mask_end_ind = df[df.d == 'd_'+str(test_edge_end)].index[-1]\n",
        "# test_mask = df.loc[test_mask_start_ind:test_mask_end_ind, :]\n",
        "\n",
        "val_edge_start = 1856 #1886-30+1\n",
        "val_edge_end = 1885 #1913-28+1\n",
        "\n",
        "val_mask_start_ind = df[df.d == 'd_'+str(val_edge_start)].index[0]\n",
        "val_mask_end_ind = df[df.d == 'd_'+str(val_edge_end)].index[-1]\n",
        "# val_mask = df.loc[val_mask_start_ind:val_mask_end_ind, :]\n",
        "\n",
        "# train_edge_start = 1095\n",
        "train_edge_start = 180\n",
        "# train_edge_start = 1676\n",
        "# train_edge_start = 1126\n",
        "train_edge_end = 1855\n",
        "\n",
        "train_mask_start_ind = df[df.d == 'd_'+str(train_edge_start)].index[0]\n",
        "train_mask_end_ind = df[df.d == 'd_'+str(train_edge_end)].index[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGv-FOHJXthU",
        "colab_type": "code",
        "outputId": "7be47cc9-e34e-45ce-a6a5-51bbd6c4348b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_test = df.loc[test_mask_start_ind:test_mask_end_ind, nums+cats+bools]\n",
        "y_test = df.loc[test_mask_start_ind:test_mask_end_ind, :]['demand']\n",
        "\n",
        "display(X_test.shape)\n",
        "display(y_test.shape)\n",
        "\n",
        "X_val = df.loc[val_mask_start_ind:val_mask_end_ind, nums+cats+bools]\n",
        "y_val = df.loc[val_mask_start_ind:val_mask_end_ind, :]['demand']\n",
        "\n",
        "display(X_val.shape)\n",
        "display(y_val.shape)\n",
        "\n",
        "X = df.loc[train_mask_start_ind:train_mask_end_ind, nums+cats+bools]\n",
        "y = df.loc[train_mask_start_ind:train_mask_end_ind, :]['demand']\n",
        "\n",
        "display(X.shape)\n",
        "display(y.shape)\n",
        "\n",
        "# display(df.shape)\n",
        "# del df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(853720, 42)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(853720,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(914700, 42)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(914700,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(51101240, 42)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(51101240,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKUAoGgl3dXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pipeline obviously\n",
        "pl = Pipeline_IMB([    \n",
        "    ('union_outer', FeatureUnion(transformer_list=[\n",
        "#         ('get_nums', Pipeline([\n",
        "#             ('get_num', get_num),\n",
        "# #             ('discretizer', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')),\n",
        "#         ])),\n",
        "        ('get_cat', Pipeline([\n",
        "            ('get_cat', get_cat),\n",
        "            ('lab_enc', OrdinalEncoder())\n",
        "            # ('ohe', OneHotEncoder(dtype='uint8', handle_unknown='ignore'))\n",
        "        ])),\n",
        "        ('get_bools', Pipeline([\n",
        "            ('get_num', get_bool),\n",
        "#             ('discretizer', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')),\n",
        "        ])),\n",
        "    ], n_jobs=1, verbose=True)),\n",
        "    \n",
        "#     ('dim_red', SelectKBest(f_regression, 5000)),\n",
        "#     ('sampling', SMOTE(sampling_strategy='auto', n_jobs=12)),\n",
        "#     ('decomp', TruncatedSVD(n_components=100)),\n",
        "#     ('dim_red', SelectKBest(chi2, 10000)),\n",
        "#     ('imp', SimpleImputer(strategy=\"median\")),\n",
        "#     ('inter', SparseInteractions(degree=2)),\n",
        "#     ('inter', PolynomialFeatures(degree=2, order='F')),\n",
        "    # ('scale', MaxAbsScaler()),\n",
        "    # ('scale', StandardScaler()),\n",
        "#     ('sfm', SelectFromModel(xgb.XGBRegressor(max_depth=100, tree_method='approx', n_jobs=16), threshold='0.5*mean')),\n",
        "#     ('dim_red', SelectKBest(chi2, 50)),\n",
        "#     ('clf', LinearRegression(n_jobs=-1))\n",
        "#     ('clf', LogisticRegression(class_weight='balanced', n_jobs=12))\n",
        "#     ('sfm', SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=3), threshold=0.015)),\n",
        "#     ('clf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=7))\n",
        "#     ('sfm', SelectFromModel(xgb.XGBRFRegressor(tree_method='hist', random_state=42, n_jobs=12), threshold=0.015)),\n",
        "#     ('xgb', MultiOutputRegressor(xgb.XGBRFRegressor(objective='reg:squarederror', max_depth=45, tree_method='hist', n_jobs=12,random_state=42)))\n",
        "#     ('xgb', xgb.XGBRegressor(max_depth=100, tree_method='approx', n_jobs=7))\n",
        "#     ('clf', LogisticRegression(class_weight='balanced', n_jobs=12))\n",
        "#     ('clf', RandomForestClassifier(class_weight='balanced', n_jobs=12))\n",
        "    # ('svc', NuSVR(degree=3, kernel='poly'))\n",
        "#     ('nn', MLPRegressor(hidden_layer_sizes=(100, 100, 100, 100)))\n",
        "#     ('svc', SVC(class_weight='balanced'))\n",
        "#     ('clf', GradientBoostingRegressor(loss= 'huber', max_depth=10))\n",
        "#     ('clf', MLPRegressor())\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH1RdDWFbkRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cat_encoder(pandas_df, cat_cols_list, dicts=None):\n",
        "    if dicts == None: dicts_ = []\n",
        "    \n",
        "    for i, col in enumerate(cat_cols_list):\n",
        "        if dicts == None:\n",
        "            dict_ = {o:i for i, o in enumerate(pandas_df[col].unique())}\n",
        "            dicts_.append(dict_)\n",
        "        else: dict_ = dicts[i]\n",
        "\n",
        "        pandas_df.loc[:, col] = pandas_df[col].apply(lambda x: dict_[x]).astype('uint16')\n",
        "    \n",
        "    return dicts\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "823xaw6qc31z",
        "colab_type": "code",
        "outputId": "bd9963f1-25af-4026-c880-ac076dbfe29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "dicts = cat_encoder(X, cats)\n",
        "cat_encoder(X_val, cats, dicts=dicts)\n",
        "cat_encoder(X_test, cats, dicts=dicts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12min 45s, sys: 790 ms, total: 12min 46s\n",
            "Wall time: 12min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zw2AX1_paZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_torch = pl.fit_transform(X)\n",
        "# X_val_torch = pl.transform(X_val)\n",
        "# X_test_torch = pl.transform(X_test)\n",
        "\n",
        "# X_torch = X\n",
        "# X_val_torch = X_val\n",
        "# X_test_torch = X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I4WqnIYaXwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 30490\n",
        "\n",
        "##### NUMS\n",
        "try:\n",
        "    X_nums_t = torch.tensor(X[nums].values)\n",
        "    X_nums_val_t = torch.tensor(X_val[nums].values, requires_grad=False)\n",
        "    X_nums_test_t = torch.tensor(X_test[nums].values, requires_grad=False)\n",
        "except:\n",
        "    X_nums_t = torch.tensor(X[nums].values.astype('float32'))\n",
        "    X_nums_val_t = torch.tensor(X_val[nums].values.astype('float32'), requires_grad=False)\n",
        "    X_nums_test_t = torch.tensor(X_test[nums].values.astype('float32'), requires_grad=False)\n",
        "##### CATS & BOOLS\n",
        "try:\n",
        "    X_cats_t = torch.tensor(X[cats+bools].values)\n",
        "    X_cats_val_t = torch.tensor(X_val[cats+bools].values, requires_grad=False)\n",
        "    X_cats_test_t = torch.tensor(X_test[cats+bools].values, requires_grad=False)\n",
        "except:\n",
        "    X_cats_t = torch.tensor(X[cats+bools].values.astype('float32'))\n",
        "    X_cats_val_t = torch.tensor(X_val[cats+bools].values.astype('float32'), requires_grad=False)\n",
        "    X_cats_test_t = torch.tensor(X_test[cats+bools].values.astype('float32'), requires_grad=False)\n",
        "\n",
        "y_torch = torch.tensor(y.values.astype('float32')).view(-1, 1)\n",
        "y_val_torch = torch.tensor(y_val.values.astype('float32')).view(-1, 1)\n",
        "y_test_torch = torch.tensor(y_test.values.astype('float32')).view(-1, 1)\n",
        "\n",
        "trainloader = DataLoader(TensorDataset(X_nums_t, X_cats_t, y_torch), batch_size=batch_size)\n",
        "validloader = DataLoader(TensorDataset(X_nums_val_t, X_cats_val_t, y_val_torch), batch_size=batch_size)\n",
        "testloader = DataLoader(TensorDataset(X_nums_test_t, X_cats_test_t, y_test_torch), batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqO9rtHFyaCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "# for i, data in enumerate(dataloader):\n",
        "#     data['image'] = data['image'].type(dtype)\n",
        "#     data['label'] = data['label'].type(dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qndlt_uYpaf",
        "colab_type": "code",
        "outputId": "cf24c374-d009-40d8-d4d1-81a5ceca3782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "def szs_def(df, columns_list):\n",
        "    szs = []\n",
        "    for col in columns_list:\n",
        "        unique = df[col].nunique()\n",
        "        dim_fun = lambda x: int(x) if (x <= 10) else int(np.sqrt(x))\n",
        "        szs.append([unique, dim_fun(unique)])\n",
        "    return szs\n",
        "\n",
        "szs = szs_def(df, df[cats+bools].columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 25 s, sys: 1.07 s, total: 26 s\n",
            "Wall time: 26.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ilypsE67gr",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHsJ7xldQ0Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def QLL(predicted, observed):\n",
        "    p = torch.tensor(1.5)\n",
        "    QLL = QLL = torch.pow(predicted, (-p))*(((predicted*observed)/(1-p)) - ((torch.pow(predicted, 2))/(2-p)))\n",
        "\n",
        "    return QLL\n",
        "        \n",
        "def tweedieloss(predicted, observed):\n",
        "    '''\n",
        "    Custom loss fuction designed to minimize the deviance using stochastic gradient descent\n",
        "    tweedie deviance from McCullagh 1983\n",
        "\n",
        "    '''\n",
        "    d = -2*QLL(predicted, observed)\n",
        "#     loss = (weight*d)/1\n",
        "\n",
        "\n",
        "    return torch.mean(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTwzOeVKPnB7",
        "colab_type": "code",
        "outputId": "9c301672-cd68-4585-cae2-ed460cef23ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "model = RNN_Emb_Model(emb_szs=szs, cont_len=len(nums))\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_Emb_Model(\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (embeddings): ModuleList(\n",
              "    (0): Embedding(30490, 174)\n",
              "    (1): Embedding(3049, 55)\n",
              "    (2): Embedding(7, 7)\n",
              "    (3): Embedding(3, 3)\n",
              "    (4): Embedding(10, 10)\n",
              "    (5): Embedding(3, 3)\n",
              "    (6): Embedding(7, 7)\n",
              "    (7): Embedding(12, 3)\n",
              "    (8): Embedding(31, 5)\n",
              "    (9): Embedding(5, 5)\n",
              "    (10): Embedding(5, 5)\n",
              "    (11): Embedding(3, 3)\n",
              "    (12): Embedding(2, 2)\n",
              "    (13): Embedding(2, 2)\n",
              "    (14): Embedding(2, 2)\n",
              "    (15): Embedding(2, 2)\n",
              "    (16): Embedding(2, 2)\n",
              "    (17): Embedding(2, 2)\n",
              "  )\n",
              "  (rnn1): RNN(24, 90, num_layers=7, dropout=0.2, bidirectional=True)\n",
              "  (fc1): Linear(in_features=472, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az0JdKTUPnJa",
        "colab_type": "code",
        "outputId": "073cf5e0-e9c8-4353-f76c-3697d73e1d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "epochs = 1\n",
        "running_loss = 0\n",
        "print_every = 15\n",
        "for epoch in range(epochs):\n",
        "        for batch_ind, (inputs_cont, inputs_cat, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Move input and label tensors to the default device/\n",
        "            inputs_cont, inputs_cat, labels = inputs_cont.to(device), inputs_cat.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logps = model.forward(inputs_cont.float(), inputs_cat.float())\n",
        "            loss = criterion(logps, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            gc.collect()\n",
        "\n",
        "            if (batch_ind+1) % print_every == 0:\n",
        "                valid_loss = 0\n",
        "                accuracy = 0\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for inputs_cont, inputs_cat, labels in validloader:\n",
        "                        inputs_cont, inputs_cat, labels = inputs_cont.to(device), inputs_cat.to(device), labels.to(device)\n",
        "\n",
        "                        logps = model.forward(inputs_cont.float(), inputs_cat.float())\n",
        "                        batch_loss = criterion(logps, labels)\n",
        "\n",
        "                        valid_loss += batch_loss.item()\n",
        "\n",
        "                        # Calculate accuracy\n",
        "                        # ps = torch.exp(logps)\n",
        "                        # top_p, top_class = ps.topk(1, dim=1)\n",
        "                        # equals = top_class == labels.view(*top_class.shape)\n",
        "                        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                      f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                      f\"Valid loss: {valid_loss/len(validloader):.3f}.. \"\n",
        "                    #   f\"Valid accuracy: {accuracy/len(validloader):.3f}\"\n",
        "                    )\n",
        "                # gpu_info = !nvidia-smi\n",
        "                # gpu_info = '\\n'.join(gpu_info)\n",
        "                # print(gpu_info)\n",
        "                model.train()\n",
        "                running_loss = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1.. Train loss: 13.138.. Valid loss: 14.606.. \n",
            "Epoch 1/1.. Train loss: 14.056.. Valid loss: 13.606.. \n",
            "Epoch 1/1.. Train loss: 11.888.. Valid loss: 12.809.. \n",
            "Epoch 1/1.. Train loss: 11.142.. Valid loss: 11.627.. \n",
            "Epoch 1/1.. Train loss: 16.424.. Valid loss: 10.173.. \n",
            "Epoch 1/1.. Train loss: 12.789.. Valid loss: 8.462.. \n",
            "Epoch 1/1.. Train loss: 10.510.. Valid loss: 7.236.. \n",
            "Epoch 1/1.. Train loss: 11.692.. Valid loss: 5.389.. \n",
            "Epoch 1/1.. Train loss: 8.063.. Valid loss: 5.380.. \n",
            "Epoch 1/1.. Train loss: 9.843.. Valid loss: 3.577.. \n",
            "Epoch 1/1.. Train loss: 9.809.. Valid loss: 3.038.. \n",
            "Epoch 1/1.. Train loss: 6.057.. Valid loss: 2.541.. \n",
            "Epoch 1/1.. Train loss: 5.554.. Valid loss: 2.842.. \n",
            "Epoch 1/1.. Train loss: 5.548.. Valid loss: 1.926.. \n",
            "Epoch 1/1.. Train loss: 3.809.. Valid loss: 1.984.. \n",
            "Epoch 1/1.. Train loss: 3.187.. Valid loss: 1.590.. \n",
            "Epoch 1/1.. Train loss: 3.101.. Valid loss: 1.510.. \n",
            "Epoch 1/1.. Train loss: 3.570.. Valid loss: 1.782.. \n",
            "Epoch 1/1.. Train loss: 2.537.. Valid loss: 1.304.. \n",
            "Epoch 1/1.. Train loss: 3.220.. Valid loss: 1.341.. \n",
            "Epoch 1/1.. Train loss: 2.662.. Valid loss: 1.154.. \n",
            "Epoch 1/1.. Train loss: 3.415.. Valid loss: 1.139.. \n",
            "Epoch 1/1.. Train loss: 4.235.. Valid loss: 1.172.. \n",
            "Epoch 1/1.. Train loss: 3.749.. Valid loss: 1.107.. \n",
            "Epoch 1/1.. Train loss: 3.252.. Valid loss: 0.942.. \n",
            "Epoch 1/1.. Train loss: 4.056.. Valid loss: 1.066.. \n",
            "Epoch 1/1.. Train loss: 3.330.. Valid loss: 0.870.. \n",
            "Epoch 1/1.. Train loss: 3.193.. Valid loss: 1.012.. \n",
            "Epoch 1/1.. Train loss: 2.976.. Valid loss: 0.833.. \n",
            "Epoch 1/1.. Train loss: 2.544.. Valid loss: 1.228.. \n",
            "Epoch 1/1.. Train loss: 1.772.. Valid loss: 0.802.. \n",
            "Epoch 1/1.. Train loss: 2.246.. Valid loss: 1.231.. \n",
            "Epoch 1/1.. Train loss: 3.080.. Valid loss: 1.906.. \n",
            "Epoch 1/1.. Train loss: 2.186.. Valid loss: 1.038.. \n",
            "Epoch 1/1.. Train loss: 4.160.. Valid loss: 0.807.. \n",
            "Epoch 1/1.. Train loss: 2.798.. Valid loss: 0.776.. \n",
            "Epoch 1/1.. Train loss: 1.670.. Valid loss: 1.091.. \n",
            "Epoch 1/1.. Train loss: 2.424.. Valid loss: 0.777.. \n",
            "Epoch 1/1.. Train loss: 1.749.. Valid loss: 0.884.. \n",
            "Epoch 1/1.. Train loss: 1.815.. Valid loss: 0.837.. \n",
            "Epoch 1/1.. Train loss: 1.632.. Valid loss: 0.595.. \n",
            "Epoch 1/1.. Train loss: 1.753.. Valid loss: 0.810.. \n",
            "Epoch 1/1.. Train loss: 1.124.. Valid loss: 0.690.. \n",
            "Epoch 1/1.. Train loss: 1.492.. Valid loss: 0.653.. \n",
            "Epoch 1/1.. Train loss: 1.176.. Valid loss: 0.633.. \n",
            "Epoch 1/1.. Train loss: 2.896.. Valid loss: 0.750.. \n",
            "Epoch 1/1.. Train loss: 2.517.. Valid loss: 0.523.. \n",
            "Epoch 1/1.. Train loss: 2.050.. Valid loss: 0.962.. \n",
            "Epoch 1/1.. Train loss: 1.918.. Valid loss: 0.567.. \n",
            "Epoch 1/1.. Train loss: 1.849.. Valid loss: 0.679.. \n",
            "Epoch 1/1.. Train loss: 4.804.. Valid loss: 0.536.. \n",
            "Epoch 1/1.. Train loss: 8.910.. Valid loss: 0.580.. \n",
            "Epoch 1/1.. Train loss: 4.778.. Valid loss: 0.981.. \n",
            "Epoch 1/1.. Train loss: 1.776.. Valid loss: 0.617.. \n",
            "Epoch 1/1.. Train loss: 1.697.. Valid loss: 0.864.. \n",
            "Epoch 1/1.. Train loss: 1.513.. Valid loss: 0.573.. \n",
            "Epoch 1/1.. Train loss: 1.629.. Valid loss: 0.575.. \n",
            "Epoch 1/1.. Train loss: 2.698.. Valid loss: 0.475.. \n",
            "Epoch 1/1.. Train loss: 1.476.. Valid loss: 0.449.. \n",
            "Epoch 1/1.. Train loss: 2.169.. Valid loss: 0.410.. \n",
            "Epoch 1/1.. Train loss: 1.263.. Valid loss: 0.789.. \n",
            "Epoch 1/1.. Train loss: 1.642.. Valid loss: 0.605.. \n",
            "Epoch 1/1.. Train loss: 1.586.. Valid loss: 0.747.. \n",
            "Epoch 1/1.. Train loss: 1.498.. Valid loss: 0.753.. \n",
            "Epoch 1/1.. Train loss: 1.200.. Valid loss: 0.709.. \n",
            "Epoch 1/1.. Train loss: 1.341.. Valid loss: 0.795.. \n",
            "Epoch 1/1.. Train loss: 1.368.. Valid loss: 0.531.. \n",
            "Epoch 1/1.. Train loss: 1.147.. Valid loss: 0.388.. \n",
            "Epoch 1/1.. Train loss: 0.945.. Valid loss: 0.753.. \n",
            "Epoch 1/1.. Train loss: 1.274.. Valid loss: 0.894.. \n",
            "Epoch 1/1.. Train loss: 1.133.. Valid loss: 0.522.. \n",
            "Epoch 1/1.. Train loss: 1.719.. Valid loss: 0.978.. \n",
            "Epoch 1/1.. Train loss: 1.091.. Valid loss: 0.605.. \n",
            "Epoch 1/1.. Train loss: 1.678.. Valid loss: 0.486.. \n",
            "Epoch 1/1.. Train loss: 1.298.. Valid loss: 0.550.. \n",
            "Epoch 1/1.. Train loss: 0.983.. Valid loss: 0.299.. \n",
            "Epoch 1/1.. Train loss: 1.084.. Valid loss: 0.476.. \n",
            "Epoch 1/1.. Train loss: 1.297.. Valid loss: 0.395.. \n",
            "Epoch 1/1.. Train loss: 1.045.. Valid loss: 0.611.. \n",
            "Epoch 1/1.. Train loss: 1.076.. Valid loss: 0.370.. \n",
            "Epoch 1/1.. Train loss: 1.042.. Valid loss: 0.665.. \n",
            "Epoch 1/1.. Train loss: 1.091.. Valid loss: 0.365.. \n",
            "Epoch 1/1.. Train loss: 0.806.. Valid loss: 0.457.. \n",
            "Epoch 1/1.. Train loss: 1.109.. Valid loss: 0.261.. \n",
            "Epoch 1/1.. Train loss: 0.936.. Valid loss: 0.379.. \n",
            "Epoch 1/1.. Train loss: 1.052.. Valid loss: 0.246.. \n",
            "Epoch 1/1.. Train loss: 1.058.. Valid loss: 0.583.. \n",
            "Epoch 1/1.. Train loss: 0.939.. Valid loss: 0.316.. \n",
            "Epoch 1/1.. Train loss: 1.161.. Valid loss: 0.441.. \n",
            "Epoch 1/1.. Train loss: 0.965.. Valid loss: 0.246.. \n",
            "Epoch 1/1.. Train loss: 0.892.. Valid loss: 0.337.. \n",
            "Epoch 1/1.. Train loss: 1.032.. Valid loss: 0.230.. \n",
            "Epoch 1/1.. Train loss: 0.868.. Valid loss: 0.438.. \n",
            "Epoch 1/1.. Train loss: 1.108.. Valid loss: 0.236.. \n",
            "Epoch 1/1.. Train loss: 1.414.. Valid loss: 0.414.. \n",
            "Epoch 1/1.. Train loss: 1.337.. Valid loss: 0.224.. \n",
            "Epoch 1/1.. Train loss: 0.998.. Valid loss: 0.327.. \n",
            "Epoch 1/1.. Train loss: 0.926.. Valid loss: 0.233.. \n",
            "Epoch 1/1.. Train loss: 1.127.. Valid loss: 0.287.. \n",
            "Epoch 1/1.. Train loss: 0.867.. Valid loss: 0.270.. \n",
            "Epoch 1/1.. Train loss: 1.105.. Valid loss: 0.330.. \n",
            "Epoch 1/1.. Train loss: 1.031.. Valid loss: 0.286.. \n",
            "Epoch 1/1.. Train loss: 0.919.. Valid loss: 0.348.. \n",
            "Epoch 1/1.. Train loss: 0.806.. Valid loss: 0.301.. \n",
            "Epoch 1/1.. Train loss: 0.792.. Valid loss: 0.274.. \n",
            "Epoch 1/1.. Train loss: 1.055.. Valid loss: 0.294.. \n",
            "Epoch 1/1.. Train loss: 0.795.. Valid loss: 0.255.. \n",
            "Epoch 1/1.. Train loss: 0.992.. Valid loss: 0.407.. \n",
            "Epoch 1/1.. Train loss: 0.994.. Valid loss: 0.261.. \n",
            "Epoch 1/1.. Train loss: 0.885.. Valid loss: 0.349.. \n",
            "Epoch 1/1.. Train loss: 0.917.. Valid loss: 0.391.. \n",
            "CPU times: user 15h 11min 40s, sys: 8min 2s, total: 15h 19min 42s\n",
            "Wall time: 15h 19min 18s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQm1AVh8o4ex",
        "colab_type": "code",
        "outputId": "35db06c2-395c-443d-8fa0-5e12cddb8258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "torch.save(model, './drive/My Drive/Colab Notebooks/model_RNN90_Emb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN_Emb_Model. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7RPrHjHsonI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN hid=30\n",
        "Epoch 1/5.. Train loss: 2.764.. Valid loss: 1.942.. \n",
        "Epoch 1/5.. Train loss: 2.572.. Valid loss: 1.782.. \n",
        "Epoch 1/5.. Train loss: 2.842.. Valid loss: 1.937.. \n",
        "Epoch 1/5.. Train loss: 2.340.. Valid loss: 1.724.. \n",
        "Epoch 1/5.. Train loss: 2.934.. Valid loss: 1.726..\n",
        "\n",
        "# RNN bidir hid=30\n",
        "Epoch 1/1.. Train loss: 1.418.. Valid loss: 0.749.. \n",
        "Epoch 1/1.. Train loss: 1.487.. Valid loss: 0.735.. \n",
        "Epoch 1/1.. Train loss: 1.530.. Valid loss: 0.754.. \n",
        "Epoch 1/1.. Train loss: 1.352.. Valid loss: 0.722.. \n",
        "Epoch 1/1.. Train loss: 1.618.. Valid loss: 0.687.. \n",
        "\n",
        "# RNN bidir hid=30 Exp1(Log1(X))\n",
        "Epoch 1/1.. Train loss: 1.627.. Valid loss: 0.927.. \n",
        "Epoch 1/1.. Train loss: 1.767.. Valid loss: 1.041.. \n",
        "Epoch 1/1.. Train loss: 1.779.. Valid loss: 0.882.. \n",
        "Epoch 1/1.. Train loss: 1.528.. Valid loss: 0.900.. \n",
        "Epoch 1/1.. Train loss: 1.877.. Valid loss: 0.921.. \n",
        "\n",
        "# RNN bidir hid=60\n",
        "Epoch 1/1.. Train loss: 0.942.. Valid loss: 0.331.. \n",
        "Epoch 1/1.. Train loss: 1.068.. Valid loss: 0.541.. \n",
        "Epoch 1/1.. Train loss: 1.153.. Valid loss: 0.342.. \n",
        "Epoch 1/1.. Train loss: 1.049.. Valid loss: 0.612.. \n",
        "Epoch 1/1.. Train loss: 1.120.. Valid loss: 0.375..  \n",
        "\n",
        "# RNN bidir hid=30 Log1(X) Exp1(y)\n",
        "Epoch 1/2.. Train loss: 2.888.. Valid loss: 1.862.. \n",
        "Epoch 1/2.. Train loss: 2.647.. Valid loss: 1.921.. \n",
        "Epoch 1/2.. Train loss: 3.305.. Valid loss: 1.624.. \n",
        "Epoch 1/2.. Train loss: 2.435.. Valid loss: 1.664.. \n",
        "Epoch 1/2.. Train loss: 3.482.. Valid loss: 1.796..\n",
        "\n",
        "# RNN bidir hid=90\n",
        "Epoch 1/1.. Train loss: 0.795.. Valid loss: 0.255.. \n",
        "Epoch 1/1.. Train loss: 0.992.. Valid loss: 0.407.. \n",
        "Epoch 1/1.. Train loss: 0.994.. Valid loss: 0.261.. \n",
        "Epoch 1/1.. Train loss: 0.885.. Valid loss: 0.349.. \n",
        "Epoch 1/1.. Train loss: 0.917.. Valid loss: 0.391.. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pFQ13RIQaZ3",
        "colab_type": "text"
      },
      "source": [
        "#Loaded models work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEEvSjABQZtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('./drive/My Drive/Colab Notebooks/model_RNN90_Emb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Nw2AVjQ5_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display(df.head())\n",
        "\n",
        "template = df[df.d=='d_180'].iloc[:,:7].copy() #    30490 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpIQ9Q7u3dYS",
        "colab_type": "code",
        "outputId": "b0d4ccc3-fed6-495a-b3d3-b9055c9bda9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "df_ = df.copy()\n",
        "d = 'd_1914'\n",
        "template.loc[:, 'd'] = d\n",
        "display(template)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5457710</th>\n",
              "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5457711</th>\n",
              "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_002</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5457712</th>\n",
              "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_003</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5457713</th>\n",
              "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_004</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5457714</th>\n",
              "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_005</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5488195</th>\n",
              "      <td>FOODS_3_823_WI_3_validation</td>\n",
              "      <td>FOODS_3_823</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5488196</th>\n",
              "      <td>FOODS_3_824_WI_3_validation</td>\n",
              "      <td>FOODS_3_824</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5488197</th>\n",
              "      <td>FOODS_3_825_WI_3_validation</td>\n",
              "      <td>FOODS_3_825</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5488198</th>\n",
              "      <td>FOODS_3_826_WI_3_validation</td>\n",
              "      <td>FOODS_3_826</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5488199</th>\n",
              "      <td>FOODS_3_827_WI_3_validation</td>\n",
              "      <td>FOODS_3_827</td>\n",
              "      <td>FOODS_3</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>WI_3</td>\n",
              "      <td>WI</td>\n",
              "      <td>d_1914</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30490 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    id        item_id    dept_id   cat_id store_id state_id       d\n",
              "5457710  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA  d_1914\n",
              "5457711  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1       CA  d_1914\n",
              "5457712  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1       CA  d_1914\n",
              "5457713  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1       CA  d_1914\n",
              "5457714  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA  d_1914\n",
              "...                                ...            ...        ...      ...      ...      ...     ...\n",
              "5488195    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS     WI_3       WI  d_1914\n",
              "5488196    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS     WI_3       WI  d_1914\n",
              "5488197    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS     WI_3       WI  d_1914\n",
              "5488198    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS     WI_3       WI  d_1914\n",
              "5488199    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS     WI_3       WI  d_1914\n",
              "\n",
              "[30490 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYhwnRYnSrkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ = pd.concat([df_, template], axis=0, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z060UaA8VHnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG7deLHaVI2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}